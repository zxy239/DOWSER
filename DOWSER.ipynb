{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad7bab-0501-42fd-9ba5-e94f0851d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"your lib\")\n",
    "\n",
    "# Please follow the official DINOv3 guidelines and configure the environment first.\n",
    "# DINOv3: https://github.com/facebookresearch/dinov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7c300-82a5-42fa-a0e4-43533517b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 1: Infrastructure and Global Configuration (Native Resolution Ready) ====\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import random, os, json, gc\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Global Paths and Parameters ---\n",
    "TRAIN_IMG_DIR   = \"../../Dataset/Seepage/train\"\n",
    "VAL_IMG_DIR     = \"../../Dataset/Seepage/val\"\n",
    "SUPPORT_JSON    = \"./train1.json\"\n",
    "FULL_TRAIN_JSON = \"../../Dataset/Seepage/train_simplified_571.json\"\n",
    "VAL_ANN_FILE    = \"../../Dataset/Seepage/val_simplified_194.json\"\n",
    "\n",
    "# Model saving paths\n",
    "TEACHER_PATH    = \"./proto_fullres.npz\"\n",
    "\n",
    "# DINO Configuration\n",
    "DINO_REPO       = \"facebookresearch/dinov3\" \n",
    "MODEL_NAME      = \"dinov3_vitl16\"\n",
    "WEIGHTS_PATH    = \"./dinov3_vitl16_pretrain.pth\" \n",
    "DEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED            = 3407\n",
    "PATCH_SIZE      = 16\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- 2. DINOv3 Feature Extractor (Optimized Version) ---\n",
    "class DINOv3Extractor:\n",
    "    def __init__(self, repo, model_name, weights, device):\n",
    "        self.device = device\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        # Note: source='local' or 'github' depends on your local environment setup.\n",
    "        # If GitHub loading is slow, consider cloning the repo and using source='local'.\n",
    "        self.model = torch.hub.load(repo, model=model_name, source='github', pretrained=False)\n",
    "        \n",
    "        if os.path.exists(weights):\n",
    "            checkpoint = torch.load(weights, map_location=\"cpu\")\n",
    "            sd = checkpoint[\"model\"] if \"model\" in checkpoint else checkpoint\n",
    "            # Weight cleaning logic to remove module/backbone prefixes\n",
    "            clean_sd = {k.replace(\"module.\", \"\").replace(\"backbone.\", \"\"): v for k, v in sd.items()}\n",
    "            self.model.load_state_dict(clean_sd, strict=False)\n",
    "            print(\"‚úÖ Weights loaded.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Weights not found at {weights}\")\n",
    "            \n",
    "        self.model.eval().to(device)\n",
    "\n",
    "    def preprocess(self, img_pil, size):\n",
    "        \"\"\"\n",
    "        Preprocessing: Resize (only if necessary) -> Tensor -> Normalize\n",
    "        size: \n",
    "          - int: Forced square resize (e.g., 768) -> Used for specific testing\n",
    "          - None: Native resolution mode (auto-align to patch size) -> Primary mode\n",
    "        \"\"\"\n",
    "        w, h = img_pil.size\n",
    "\n",
    "        if size is not None:\n",
    "            # Forced mode\n",
    "            img = TF.resize(img_pil, (size, size), interpolation=TF.InterpolationMode.BICUBIC)\n",
    "        else:\n",
    "            # Native mode: Resize only if dimensions are not multiples of 16\n",
    "            # For 1600x1200, new_h=1200, new_w=1600; no resize will be triggered.\n",
    "            new_h = (h // PATCH_SIZE) * PATCH_SIZE\n",
    "            new_w = (w // PATCH_SIZE) * PATCH_SIZE\n",
    "            \n",
    "            if new_h != h or new_w != w:\n",
    "                # Interpolate only when alignment is required\n",
    "                img = TF.resize(img_pil, (new_h, new_w), interpolation=TF.InterpolationMode.BICUBIC)\n",
    "            else:\n",
    "                # Perfect dimensions: use original image to retain 100% sharpness\n",
    "                img = img_pil\n",
    "            \n",
    "        img = TF.to_tensor(img)\n",
    "        img = TF.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        return img.unsqueeze(0).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_patch_features(self, img_pil, size=None):\n",
    "        \"\"\"\n",
    "        Extract patch-level features\n",
    "        Returns: norm_patches [N, 1024], (H, W)\n",
    "        \"\"\"\n",
    "        x = self.preprocess(img_pil, size)\n",
    "        \n",
    "        # Use get_intermediate_layers to retrieve features\n",
    "        # n=1 indicates the last layer; reshape=True returns (B, C, H, W)\n",
    "        out = self.model.get_intermediate_layers(x, n=1, reshape=True, norm=True)[0]\n",
    "        \n",
    "        B, D, H, W = out.shape\n",
    "        # Flatten features: (H*W, D)\n",
    "        patches = out.squeeze(0).permute(1, 2, 0).reshape(H*W, D)\n",
    "        return F.normalize(patches, dim=1), (H, W)\n",
    "\n",
    "# Initialization\n",
    "if 'extractor' not in locals():\n",
    "    extractor = DINOv3Extractor(DINO_REPO, MODEL_NAME, WEIGHTS_PATH, DEVICE)\n",
    "\n",
    "# --- 3. Base Dataset Class (with orientation correction) ---\n",
    "class BaseDataset:\n",
    "    def __init__(self, img_dir, json_file):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(json_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        \n",
    "    def get_image(self, idx):\n",
    "        \"\"\"Returns the PIL image and filename (automatically corrected for orientation)\"\"\"\n",
    "        info = self.coco.loadImgs(self.img_ids[idx])[0]\n",
    "        path = os.path.join(self.img_dir, info['file_name'])\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        # Critical: Handle EXIF orientation tags to prevent vertical images from being read horizontally\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        \n",
    "        return img, info['file_name']\n",
    "\n",
    "print(\"Native Resolution Infrastructure Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff8776-e94d-4d31-926e-c4dd239980f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a0169-e248-499f-bf13-bd137b62aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 2: Native Multi-Scale Feature Mining ====\n",
    "# Strategy:\n",
    "# 1. Scale Augmentation: Enables model to perceive seepage textures at varying sizes.\n",
    "# 2. Memory Safety: Automatic random cropping during upscaling to prevent OOM errors.\n",
    "# 3. Grid Alignment: Forces dimensions to be multiples of 16 to reduce edge artifacts.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# --- 1. Parameters ---\n",
    "DATA_JSON  = SUPPORT_JSON \n",
    "TARGET_ID  = 1   # Seepage\n",
    "JOINT_ID   = 2   # Joint\n",
    "STRUCT_IDS = [3, 4, 5]\n",
    "\n",
    "# Increased rounds to account for added sample diversity from scaling\n",
    "AUG_ROUNDS = 9 \n",
    "SEED = 3407 \n",
    "PATCH_SIZE = 16\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(f\"üîÑ Resetting Seed to {SEED}...\")\n",
    "set_seed(SEED) \n",
    "\n",
    "# --- 2. Robust Multi-Scale Augmentation ---\n",
    "def augment_pair_robust(img, mask):\n",
    "    w, h = img.size\n",
    "    \n",
    "    # === A. Random Scaling ===\n",
    "    # Range: 0.8 (Global view) ~ 1.5 (Detailed view)\n",
    "    scale = random.uniform(0.8, 1.5)\n",
    "    \n",
    "    # Calculate new dimensions (align to Patch Size)\n",
    "    new_h = int(h * scale)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = (new_h // PATCH_SIZE) * PATCH_SIZE\n",
    "    new_w = (new_w // PATCH_SIZE) * PATCH_SIZE\n",
    "    \n",
    "    # Size fallback\n",
    "    if new_h < 512 or new_w < 512:\n",
    "        new_h, new_w = h, w \n",
    "    \n",
    "    # Resize (Bicubic for Image, Nearest for Mask)\n",
    "    img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "    mask = Image.fromarray(mask).resize((new_w, new_h), Image.NEAREST)\n",
    "    \n",
    "    # === B. Random Cropping (Memory Protection) ===\n",
    "    # Trigger crop if pixels exceed 1600x1200 or based on probability\n",
    "    max_pixels = 1600 * 1200\n",
    "    curr_pixels = new_w * new_h\n",
    "    \n",
    "    if curr_pixels > max_pixels or random.random() > 0.3:\n",
    "        # Define crop size (multiples of 16)\n",
    "        crop_h = min(new_h, random.choice([768, 896, 1024]))\n",
    "        crop_w = min(new_w, random.choice([768, 896, 1024]))\n",
    "        \n",
    "        if new_h > crop_h and new_w > crop_w:\n",
    "            top = random.randint(0, new_h - crop_h)\n",
    "            left = random.randint(0, new_w - crop_w)\n",
    "            \n",
    "            img = img.crop((left, top, left + crop_w, top + crop_h))\n",
    "            mask = mask.crop((left, top, left + crop_w, top + crop_h))\n",
    "    \n",
    "    # === C. Geometric Transforms (Flip & Rotate) ===\n",
    "    if random.random() > 0.5:\n",
    "        img = TF.hflip(img); mask = TF.hflip(mask)\n",
    "    if random.random() > 0.5:\n",
    "        img = TF.vflip(img); mask = TF.vflip(mask)\n",
    "    \n",
    "    # 90-degree rotations\n",
    "    if random.random() > 0.3:\n",
    "        angle = random.choice([90, 180, 270])\n",
    "        img = TF.rotate(img, angle)\n",
    "        mask = TF.rotate(mask, angle)\n",
    "        \n",
    "    return img, np.array(mask)\n",
    "\n",
    "# --- 3. Initialize Containers ---\n",
    "feats_fg = []\n",
    "feats_joint = []\n",
    "feats_struct = []\n",
    "feats_normal = [] \n",
    "\n",
    "# --- 4. Feature Extraction ---\n",
    "print(f\"üöÄ Extracting Multi-Scale Features...\")\n",
    "\n",
    "if 'BaseDataset' not in globals(): raise RuntimeError(\"Please run Cell 1 first\")\n",
    "def get_len(self): return len(self.img_ids)\n",
    "BaseDataset.__len__ = get_len\n",
    "    \n",
    "ds = BaseDataset(TRAIN_IMG_DIR, DATA_JSON)\n",
    "model = extractor.model\n",
    "model.eval()\n",
    "\n",
    "for idx in range(len(ds)):\n",
    "    img_orig, fname = ds.get_image(idx) # Orientation corrected\n",
    "    \n",
    "    # Parse GT Mask\n",
    "    w, h = img_orig.size \n",
    "    mask_orig = np.zeros((h, w), dtype=np.uint8)\n",
    "    ann_ids = ds.coco.getAnnIds(imgIds=ds.img_ids[idx])\n",
    "    \n",
    "    for ann in ds.coco.loadAnns(ann_ids):\n",
    "        m = ds.coco.annToMask(ann)\n",
    "        if m.shape != (h, w): # Size safety check\n",
    "            m = np.array(Image.fromarray(m).resize((w, h), Image.NEAREST))\n",
    "            \n",
    "        cid = ann['category_id']\n",
    "        if cid == TARGET_ID: mask_orig[m>0] = 1\n",
    "        elif cid == JOINT_ID: mask_orig[m>0] = 2\n",
    "        elif cid in STRUCT_IDS: mask_orig[m>0] = 3\n",
    "    \n",
    "    # üîÅ Augmentation Loop\n",
    "    for r in range(AUG_ROUNDS + 1):\n",
    "        if r == 0:\n",
    "            # First round: Use original image (no scale/crop)\n",
    "            curr_img, curr_mask = img_orig, mask_orig\n",
    "        else:\n",
    "            # Subsequent rounds: Retry mechanism to ensure key features are captured\n",
    "            for _ in range(5): \n",
    "                res_img, res_mask = augment_pair_robust(img_orig, mask_orig)\n",
    "                # Ensure either FG or Joint exists in the crop\n",
    "                if 1 in res_mask or 2 in res_mask:\n",
    "                    curr_img, curr_mask = res_img, res_mask\n",
    "                    break\n",
    "            else:\n",
    "                # Skip round if key features aren't captured after 5 attempts\n",
    "                continue\n",
    "        \n",
    "        # 1. Feature Extraction (size=None -> auto-adapt to current resolution)\n",
    "        try:\n",
    "            feat_map, (hf, wf) = extractor.get_patch_features(curr_img, size=None)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"‚ö†Ô∏è OOM skipping round {r}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            continue\n",
    "            \n",
    "        feat_map = feat_map.cpu()\n",
    "\n",
    "        # 3. Probability Map Generation\n",
    "        # Note: (hf, wf) is dynamic across rounds\n",
    "        def get_prob_map(val_id):\n",
    "            mask_float = (curr_mask == val_id).astype(np.float32)\n",
    "            t = torch.from_numpy(mask_float).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "            # Bilinear interpolation for soft labels\n",
    "            prob = F.interpolate(t, size=(hf, wf), mode='bilinear', align_corners=False).flatten()\n",
    "            return prob.cpu()\n",
    "\n",
    "        prob_fg = get_prob_map(1)\n",
    "        prob_joint = get_prob_map(2)\n",
    "        prob_struct = get_prob_map(3)\n",
    "        \n",
    "        # 4. Selection Logic\n",
    "        # FG: High confidence seepage area\n",
    "        idx_fg = (prob_fg > 0.6) & (prob_joint < 0.1) \n",
    "        \n",
    "        # Joint\n",
    "        idx_joint = (prob_joint > 0.4)\n",
    "        \n",
    "        # Struct\n",
    "        idx_struct = (prob_struct > 0.6)\n",
    "        \n",
    "        # Normal (Background)\n",
    "        idx_normal = (prob_fg < 0.05) & (prob_joint < 0.05) & (prob_struct < 0.05)\n",
    "        \n",
    "        # 5. Collection\n",
    "        if idx_fg.any():     feats_fg.append(feat_map[idx_fg])\n",
    "        if idx_joint.any():  feats_joint.append(feat_map[idx_joint])\n",
    "        if idx_struct.any(): feats_struct.append(feat_map[idx_struct])\n",
    "        \n",
    "        # Background downsampling\n",
    "        curr_norm = feat_map[idx_normal]\n",
    "        if curr_norm.shape[0] > 3000:\n",
    "            perm = torch.randperm(curr_norm.shape[0])[:3000]\n",
    "            curr_norm = curr_norm[perm]\n",
    "        if curr_norm.shape[0] > 0:\n",
    "            feats_normal.append(curr_norm)\n",
    "\n",
    "    print(f\"   Processed {idx+1}/{len(ds)} images...\")\n",
    "\n",
    "print(f\"‚úÖ Multi-Scale Extraction Done!\")\n",
    "\n",
    "# Statistics\n",
    "total_fg = sum(len(x) for x in feats_fg)\n",
    "total_joint = sum(len(x) for x in feats_joint)\n",
    "total_struct = sum(len(x) for x in feats_struct)\n",
    "total_normal = sum(len(x) for x in feats_normal)\n",
    "\n",
    "print(f\"üìä Final Stats (Multi-Scale):\")\n",
    "print(f\"   - FG Blocks:     {total_fg}\")\n",
    "print(f\"   - Joint Blocks:  {total_joint}\")\n",
    "print(f\"   - Struct Blocks: {total_struct}\")\n",
    "print(f\"   - Normal Blocks: {total_normal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9195f0-e727-4a31-a5f9-e7c63f07330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 3: Auto-Optimal K Search (Prototype Adaptation) ====\n",
    "# Goal: Analyze feature space geometry to calculate the optimal number of cluster centers without inference.\n",
    "# Method: Maximum Curvature (Kneedle Algorithm) / Elbow Method.\n",
    "# Use Case: Handling massive feature sets under Native Resolution.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Range Configuration ---\n",
    "# In Native Mode, features are more diverse; ranges are widened accordingly.\n",
    "# FG (Foreground): Seepage shapes vary (cracks, patches, spots), requiring sufficient K to cover.\n",
    "FG_RANGE = [16, 32, 64, 80, 96, 128]\n",
    "\n",
    "# BG (Background): Includes joints, structures, and concrete under various lighting‚Äîextremely high complexity.\n",
    "BG_RANGE = [32, 64, 96, 128, 160, 192, 256, 320]\n",
    "\n",
    "# Save path (automatically overwrites old prototype files)\n",
    "SAVE_PATH = TEACHER_PATH\n",
    "\n",
    "# --- 2. Core Algorithm: Mathematical \"Inflection Point\" Search ---\n",
    "def find_optimal_k(feat_list, k_range, label_name):\n",
    "    # 1. Data Preparation\n",
    "    if not feat_list:\n",
    "        print(f\"‚ö†Ô∏è {label_name} is empty!\")\n",
    "        return 1, np.zeros((1, 1024))\n",
    "    \n",
    "    # Convert to Numpy (CPU)\n",
    "    pool = torch.cat(feat_list, dim=0).numpy()\n",
    "    N = pool.shape[0]\n",
    "    \n",
    "    # Filter invalid K (must be less than total samples)\n",
    "    valid_k_range = [k for k in k_range if k < N]\n",
    "    if not valid_k_range:\n",
    "        valid_k_range = [N] # If samples are extremely few, K = sample count\n",
    "    \n",
    "    print(f\"\\nüîç Analyzing {label_name} (Total Samples: {N})...\")\n",
    "    \n",
    "    inertias = []\n",
    "    models = {}\n",
    "    \n",
    "    # 2. Iteratively calculate Inertia (Within-cluster Sum of Squared errors)\n",
    "    # Inertia decreases as K increases; we seek the point where the rate of decrease slows significantly.\n",
    "    for k in tqdm(valid_k_range, leave=False, desc=f\"Searching {label_name}\"):\n",
    "        kmeans = MiniBatchKMeans(\n",
    "            n_clusters=k, \n",
    "            batch_size=4096, \n",
    "            n_init=\"auto\", \n",
    "            random_state=3407\n",
    "        ).fit(pool)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        models[k] = kmeans.cluster_centers_\n",
    "\n",
    "    # 3. Mathematical \"Elbow Point\" Calculation\n",
    "    # Normalize the curve to a [0, 1] square and calculate the distance from each point to the start-end line.\n",
    "    x = np.array(range(len(inertias)))\n",
    "    y = np.array(inertias)\n",
    "    \n",
    "    # Linear Normalization\n",
    "    if len(x) > 1:\n",
    "        x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "        y_norm = (y - y.min()) / (y.max() - y.min())\n",
    "    else:\n",
    "        return valid_k_range[0], models[valid_k_range[0]]\n",
    "    \n",
    "    # Vector geometry to calculate distance\n",
    "    coords = np.vstack((x_norm, y_norm)).T\n",
    "    p1, p2 = coords[0], coords[-1] # Start and End points\n",
    "    vec = p2 - p1\n",
    "    vec_norm = np.linalg.norm(vec)\n",
    "    \n",
    "    distances = []\n",
    "    for p in coords:\n",
    "        # Cross product to find height (distance) from the line segment\n",
    "        # The point with maximum distance is the Elbow point.\n",
    "        dist = np.abs(np.cross(vec, p - p1)) / (vec_norm + 1e-8)\n",
    "        distances.append(dist)\n",
    "        \n",
    "    # Locate index with maximum distance\n",
    "    best_idx = np.argmax(distances)\n",
    "    best_k = valid_k_range[best_idx]\n",
    "    \n",
    "    # Print Analysis Table\n",
    "    print(f\"{'K':<5} | {'Inertia':<12} | {'Curvature':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, k in enumerate(valid_k_range):\n",
    "        mark = \"üëà Best (Elbow)\" if k == best_k else \"\"\n",
    "        print(f\"{k:<5} | {inertias[i]:<12.1f} | {distances[i]:<10.4f} {mark}\")\n",
    "        \n",
    "    return best_k, models[best_k]\n",
    "\n",
    "# --- 3. Execution and Saving ---\n",
    "\n",
    "# Combine background features: Joint + Struct + Normal\n",
    "all_bg_feats = feats_joint + feats_struct + feats_normal\n",
    "\n",
    "print(\"üöÄ Starting Automatic K Optimization...\")\n",
    "\n",
    "# A. Analyze Foreground (FG)\n",
    "best_k_fg, centers_fg = find_optimal_k(feats_fg, FG_RANGE, \"Foreground\")\n",
    "\n",
    "# B. Analyze Background (BG)\n",
    "best_k_bg, centers_bg = find_optimal_k(all_bg_feats, BG_RANGE, \"Background\")\n",
    "\n",
    "# C. Post-processing and Storage\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÜ Optimization Complete\")\n",
    "print(f\"   FG Best K: {best_k_fg}\")\n",
    "print(f\"   BG Best K: {best_k_bg}\")\n",
    "\n",
    "# Convert to Tensor and perform L2 Normalization (Mandatory for DINOv3)\n",
    "p_fg_final = torch.from_numpy(centers_fg).float()\n",
    "p_bg_final = torch.from_numpy(centers_bg).float()\n",
    "\n",
    "p_fg_final = F.normalize(p_fg_final, dim=1)\n",
    "p_bg_final = F.normalize(p_bg_final, dim=1)\n",
    "\n",
    "# Save results\n",
    "np.savez(SAVE_PATH, fg=p_fg_final.numpy(), bg=p_bg_final.numpy())\n",
    "\n",
    "print(f\"üíæ Optimal Prototypes Saved to: {SAVE_PATH}\")\n",
    "print(f\"   - FG Shape: {p_fg_final.shape}\")\n",
    "print(f\"   - BG Shape: {p_bg_final.shape}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f82be6-df56-49d8-8f73-be1bf840b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 3.5: Re-clustering ====\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "OVERRIDE_FG_K = best_k_fg \n",
    "OVERRIDE_BG_K = best_k_bg\n",
    "\n",
    "SAVE_PATH = TEACHER_PATH\n",
    "\n",
    "print(f\"üîß Manually Overriding Prototypes...\")\n",
    "print(f\"   FG: {best_k_fg} -> {OVERRIDE_FG_K} (Boosting coverage)\")\n",
    "print(f\"   BG: {best_k_bg} -> {OVERRIDE_BG_K} (Maintaining optimal)\")\n",
    "\n",
    "# --- 2. Re-clustering ---\n",
    "def fast_cluster(feat_list, k):\n",
    "    if not feat_list: return torch.zeros((0, 1024))\n",
    "    pool = torch.cat(feat_list, dim=0).numpy()\n",
    "    \n",
    "    # Safety check: ensure K does not exceed total number of samples\n",
    "    real_k = min(k, pool.shape[0])\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=real_k, \n",
    "        batch_size=4096, \n",
    "        n_init=\"auto\", \n",
    "        random_state=3407\n",
    "    ).fit(pool)\n",
    "    \n",
    "    centers = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "    return F.normalize(centers, dim=1)\n",
    "\n",
    "# Re-generate prototypes\n",
    "p_fg_new = fast_cluster(feats_fg, OVERRIDE_FG_K)\n",
    "p_bg_new = fast_cluster(all_bg_feats, OVERRIDE_BG_K) # all_bg_feats inherited from previous cell\n",
    "\n",
    "# --- 3. Save to Disk ---\n",
    "np.savez(SAVE_PATH, fg=p_fg_new.numpy(), bg=p_bg_new.numpy())\n",
    "\n",
    "print(f\"\\n‚úÖ Overridden Prototypes Saved to: {SAVE_PATH}\")\n",
    "print(f\"   - FG Shape: {p_fg_new.shape}\")\n",
    "print(f\"   - BG Shape: {p_bg_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0c504-079b-4ea6-88c9-b7ad8bcd75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4_TTA_Fixed_Base: Stable Target Resolution TTA (V13) ====\n",
    "\n",
    "import os\n",
    "# üî• Maintain VRAM configuration to prevent fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import time\n",
    "import gc\n",
    "import skimage.morphology as morph \n",
    "import random # For random rotation TTA\n",
    "\n",
    "# ==========================================\n",
    "# 1. Core Configuration\n",
    "# ==========================================\n",
    "PROTO_PATH  = \"./proto_fullres.npz\"\n",
    "FINAL_TH    = 0.50  \n",
    "PATCH_SIZE  = 16\n",
    "# üî• Fixed TTA Target Base Resolution\n",
    "BASE_H, BASE_W = 1200, 1600 \n",
    "\n",
    "TTA_CONFIG = [\n",
    "    # (scale, flip, rotation_degree)\n",
    "    (1.0, False, 0),         # 1. Base 1.0x\n",
    "    # (1.0, True, 0),          # 2. Flip 1.0x\n",
    "    # (0.75, False, 0),        # 3. Base 0.75x\n",
    "    # (0.75, True, 0),         # 4. Flip 0.75x\n",
    "    # (1.0, False, 5),         # 5. Rotation +5 deg\n",
    "    # (1.0, False, -5),        # 6. Rotation -5 deg\n",
    "]\n",
    "\n",
    "# Load Prototypes\n",
    "pd = np.load(PROTO_PATH)\n",
    "p_fg = torch.from_numpy(pd['fg']).float().to(DEVICE)\n",
    "p_bg = torch.from_numpy(pd['bg']).float().to(DEVICE)\n",
    "p_fg = F.normalize(p_fg, dim=1)\n",
    "p_bg = F.normalize(p_bg, dim=1)\n",
    "\n",
    "K_FG, K_BG = 32, 96\n",
    "\n",
    "print(f\"üöÄ Starting Fixed Base TTA Inference (Base: {BASE_H}x{BASE_W}, Passes: {len(TTA_CONFIG)})...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Preprocessing & Environment Setup\n",
    "# ==========================================\n",
    "ds_val = BaseDataset(VAL_IMG_DIR, VAL_ANN_FILE)\n",
    "extractor.model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Single Inference Function (Serial Processing for Stability)\n",
    "# ==========================================\n",
    "def infer_single_fixed_base(img_pil_orig, scale, flip, rotation_deg):\n",
    "    w_orig, h_orig = img_pil_orig.size\n",
    "    \n",
    "    # üî• Fix: Target dimensions based on BASE_H/W and scale factor\n",
    "    h_target_scaled = (int(BASE_H * scale) // PATCH_SIZE) * PATCH_SIZE\n",
    "    w_target_scaled = (int(BASE_W * scale) // PATCH_SIZE) * PATCH_SIZE\n",
    "    \n",
    "    # Adaptive Portrait/Landscape orientation alignment\n",
    "    if h_orig > w_orig:\n",
    "        # Portrait (H > W): Force portrait target\n",
    "        h_in, w_in = max(h_target_scaled, w_target_scaled), min(h_target_scaled, w_target_scaled)\n",
    "    else:\n",
    "        # Landscape (W > H) or Square: Force landscape target\n",
    "        h_in, w_in = min(h_target_scaled, w_target_scaled), max(h_target_scaled, w_target_scaled)\n",
    "        \n",
    "    img_input = img_pil_orig\n",
    "    \n",
    "    # TTA Augmentations\n",
    "    if flip: \n",
    "        img_input = ImageOps.mirror(img_input)\n",
    "    if rotation_deg != 0:\n",
    "        img_input = img_input.rotate(rotation_deg, resample=Image.BICUBIC, expand=False)\n",
    "        \n",
    "    img_tensor = None; out = None; prob_map = None\n",
    "    \n",
    "    try:\n",
    "        # 1. Force Resize to calculated target dimensions\n",
    "        img_input = img_input.resize((w_in, h_in), Image.BICUBIC)\n",
    "        \n",
    "        # 2. Preprocess\n",
    "        img_tensor = extractor.preprocess(img_input, size=None)\n",
    "        \n",
    "        # 3. Forward Pass\n",
    "        with torch.inference_mode():\n",
    "            out = extractor.model.get_intermediate_layers(img_tensor, n=1, reshape=True, norm=True)[0]\n",
    "            \n",
    "            B, D, H_feat, W_feat = out.shape\n",
    "            feat = out.squeeze(0).permute(1, 2, 0).reshape(-1, D)\n",
    "            feat = F.normalize(feat, dim=1)\n",
    "            \n",
    "            # Similarity Matching\n",
    "            sim_fg = torch.mm(feat, p_fg.t()); val_fg, _ = sim_fg.topk(K_FG, dim=1)\n",
    "            sim_bg = torch.mm(feat, p_bg.t()); val_bg, _ = sim_bg.topk(K_BG, dim=1)\n",
    "            \n",
    "            logits = torch.cat([val_fg, val_bg], dim=1); probs = F.softmax(logits / 0.07, dim=1)\n",
    "            score_fg = probs[:, :K_FG].sum(dim=1)\n",
    "            \n",
    "            score_map = score_fg.view(1, 1, H_feat, W_feat)\n",
    "            \n",
    "            # Interpolate back to original resolution\n",
    "            prob_map = F.interpolate(score_map, size=(h_orig, w_orig), mode='bilinear', align_corners=False).squeeze()\n",
    "            \n",
    "            prob_map_np = prob_map.cpu().numpy()\n",
    "            \n",
    "            # 4. Reverse Augmentations (Rotation/Flip)\n",
    "            if flip:\n",
    "                prob_map_np = np.fliplr(prob_map_np)\n",
    "            if rotation_deg != 0:\n",
    "                # Rotate back using PIL (Nearest interpolation ensures pixel alignment)\n",
    "                prob_map_np = np.array(Image.fromarray((prob_map_np * 255).astype(np.uint8)).rotate(-rotation_deg, resample=Image.NEAREST, expand=False))/255.0\n",
    "\n",
    "            return prob_map_np\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Runtime Error at {scale}x scale: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up Python objects within TTA iteration\n",
    "        del img_tensor, out, prob_map\n",
    "        if 'feat' in locals(): del feat\n",
    "        gc.collect() \n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Main Inference Loop\n",
    "# ==========================================\n",
    "pbar = tqdm(range(len(ds_val.img_ids)), desc=\"Fixed Base TTA\", unit=\"img\")\n",
    "running_mious = []; running_fg_ious = []; results_dict = {} \n",
    "start_total_time = time.time()\n",
    "\n",
    "for idx in pbar:\n",
    "    img_pil, fname = ds_val.get_image(idx) \n",
    "    w_real, h_real = img_pil.size\n",
    "    \n",
    "    # Prepare Ground Truth (GT)\n",
    "    gt = np.zeros((h_real, w_real), dtype=np.uint8)\n",
    "    ann_ids = ds_val.coco.getAnnIds(imgIds=ds_val.img_ids[idx])\n",
    "    for ann in ds_val.coco.loadAnns(ann_ids):\n",
    "        if ann['category_id'] == 1: \n",
    "            m = ds_val.coco.annToMask(ann)\n",
    "            if m.shape != (h_real, w_real): \n",
    "                m = np.array(Image.fromarray(m).resize((w_real, h_real), Image.NEAREST))\n",
    "            gt = np.maximum(gt, m)\n",
    "\n",
    "    # --- TTA Loop ---\n",
    "    accum_prob_np = np.zeros((h_real, w_real), dtype=np.float32)\n",
    "    valid_count = 0\n",
    "    current_img_time = time.time()\n",
    "    \n",
    "    for scale, flip, rotation_deg in TTA_CONFIG:\n",
    "        res = infer_single_fixed_base(img_pil, scale, flip, rotation_deg)\n",
    "        if res is not None: \n",
    "            accum_prob_np += res\n",
    "            valid_count += 1\n",
    "            \n",
    "    # --- Final Mask Generation & Metrics ---\n",
    "    miou, fg_iou = 0.0, 0.0\n",
    "    \n",
    "    if valid_count > 0:\n",
    "        p_final = accum_prob_np / valid_count\n",
    "        final_mask = (p_final > FINAL_TH).astype(np.uint8)\n",
    "        \n",
    "        # Morphological Post-processing\n",
    "        if final_mask.sum() > 0: \n",
    "            final_mask = morph.remove_small_objects(final_mask.astype(bool), min_size=50).astype(np.uint8)\n",
    "            \n",
    "        # IoU Computation\n",
    "        i_area = ((final_mask==1)&(gt==1)).sum()\n",
    "        u_area = ((final_mask==1)|(gt==1)).sum()\n",
    "        curr_fg = i_area / (u_area + 1e-6)\n",
    "        \n",
    "        bg_i = ((final_mask==0)&(gt==0)).sum()\n",
    "        bg_u = ((final_mask==0)|(gt==0)).sum()\n",
    "        curr_miou = (curr_fg + bg_i/(bg_u+1e-6))/2.0\n",
    "        \n",
    "        miou = curr_miou; fg_iou = curr_fg\n",
    "    \n",
    "    # Store results\n",
    "    results_dict[idx] = {'miou': miou, 'fg': fg_iou}\n",
    "    running_mious.append(miou); running_fg_ious.append(fg_iou)\n",
    "    \n",
    "    # Progress Display Updates\n",
    "    total_processed = idx + 1\n",
    "    avg_total_time = (time.time() - start_total_time) / total_processed\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        \"mIoU\": f\"{np.mean(running_mious):.4f}\", \n",
    "        \"T/img\": f\"{time.time() - current_img_time:.2f}s\",\n",
    "        \"Avg T\": f\"{avg_total_time:.2f}s\"\n",
    "    })\n",
    "\n",
    "    # üî• Critical: Clear VRAM cache only when switching images\n",
    "    del accum_prob_np, gt\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "# ==========================================\n",
    "# 5. Final Statistics\n",
    "# ==========================================\n",
    "total_time_taken = time.time() - start_total_time\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÜ Final Fixed Base TTA Results (Total {len(running_mious)} samples):\")\n",
    "print(f\"   mIoU:   {np.mean(running_mious):.4f}\")\n",
    "print(f\"   FG IoU: {np.mean(running_fg_ious):.4f}\")\n",
    "print(f\"   Total Time: {total_time_taken/60:.2f} min ({total_time_taken:.2f} sec)\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c25dc-d9e3-4d95-898d-777ea17843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c3e2d-70b4-45f2-b6ac-0f0c89dfe105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 5: MLP-based Feature Extraction & Smart Mining ====\n",
    "# 1. \"Retry Mechanism\": Ensures augmented crops contain seepage or joints to avoid wasted iterations.\n",
    "# 2. VRAM Protection: Features are moved to CPU immediately after extraction.\n",
    "# 3. Dimensional Alignment: Forced size=768 to ensure consistent DINOv3 patch ratios.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "DATA_JSON  = SUPPORT_JSON \n",
    "TARGET_ID  = 1   # Seepage\n",
    "JOINT_ID   = 2   # Joint\n",
    "STRUCT_IDS = [3, 4, 5]\n",
    "\n",
    "AUG_ROUNDS = 9 \n",
    "SEED = 3407 \n",
    "\n",
    "# Reset Seed for Reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(f\"üîÑ Resetting Seed to {SEED}...\")\n",
    "set_seed(SEED) \n",
    "\n",
    "# --- 2. Augmentation Function ---\n",
    "def augment_pair_multiclass(img, mask):\n",
    "    # Random Cropping\n",
    "    if random.random() > 0.3:\n",
    "        w, h = img.size; scale = random.uniform(0.4, 1.0)\n",
    "        nh, nw = int(h*scale), int(w*scale)\n",
    "        top = random.randint(0, h-nh); left = random.randint(0, w-nw)\n",
    "        img = TF.crop(img, top, left, nh, nw)\n",
    "        mask = TF.crop(Image.fromarray(mask), top, left, nh, nw); mask = np.array(mask)\n",
    "    # Flipping\n",
    "    if random.random() > 0.5: img = TF.hflip(img); mask = np.fliplr(mask)\n",
    "    if random.random() > 0.5: img = TF.vflip(img); mask = np.flipud(mask)\n",
    "    # Rotation\n",
    "    if random.random() > 0.125:\n",
    "        angle = random.choice([45,90,135,180,225,270,315])\n",
    "        img = TF.rotate(img, angle); mask = np.array(TF.rotate(Image.fromarray(mask), angle))\n",
    "    return img, mask\n",
    "\n",
    "# --- 3. Initialize Containers ---\n",
    "feats_fg = []\n",
    "feats_joint = []\n",
    "feats_struct = []\n",
    "feats_normal = [] \n",
    "\n",
    "# --- 4. Feature Extraction (Smart Mining Loop) ---\n",
    "print(f\"üöÄ Extracting Features (Smart Mining Strategy)...\")\n",
    "\n",
    "# Dataset sanity check\n",
    "if 'BaseDataset' not in globals(): raise RuntimeError(\"Please run Cell 1 first.\")\n",
    "def get_len(self): return len(self.img_ids)\n",
    "BaseDataset.__len__ = get_len\n",
    "    \n",
    "ds = BaseDataset(TRAIN_IMG_DIR, DATA_JSON)\n",
    "model = extractor.model\n",
    "model.eval()\n",
    "\n",
    "for idx in range(len(ds)):\n",
    "    img_orig, fname = ds.get_image(idx)\n",
    "    \n",
    "    # Parse Ground Truth (GT) Mask\n",
    "    w, h = img_orig.size; mask_orig = np.zeros((h, w), dtype=np.uint8)\n",
    "    ann_ids = ds.coco.getAnnIds(imgIds=ds.img_ids[idx])\n",
    "    for ann in ds.coco.loadAnns(ann_ids):\n",
    "        m = ds.coco.annToMask(ann)\n",
    "        cid = ann['category_id']\n",
    "        if cid == TARGET_ID: mask_orig[m>0] = 1\n",
    "        elif cid == JOINT_ID: mask_orig[m>0] = 2\n",
    "        elif cid in STRUCT_IDS: mask_orig[m>0] = 3\n",
    "    \n",
    "    # üîÅ Augmentation Loop\n",
    "    for r in range(AUG_ROUNDS + 1):\n",
    "        # Strategy: Preserve original image in Round 0, use smart augmentation thereafter\n",
    "        if r == 0:\n",
    "            curr_img, curr_mask = img_orig, mask_orig\n",
    "        else:\n",
    "            # üî• Core Improvement: Smart Retry Mechanism\n",
    "            # Attempt 3 times to crop an area containing Seepage (1) or Joints (2)\n",
    "            # Proceed only if critical targets are found or retries are exhausted\n",
    "            for _ in range(3):\n",
    "                curr_img, curr_mask = augment_pair_multiclass(img_orig, mask_orig)\n",
    "                if 1 in curr_mask or 2 in curr_mask: \n",
    "                    break\n",
    "        \n",
    "        # 1. Feature Extraction (Fixed size 768 for consistent Patch ratios)\n",
    "        try:\n",
    "            feat_map, (hf, wf) = extractor.get_patch_features(curr_img, size=768)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        # 2. ‚ö° Immediate Transfer to CPU (VRAM Protection)\n",
    "        feat_map = feat_map.cpu()\n",
    "\n",
    "        # 3. Probability Map Generation (Helper function)\n",
    "        def get_prob_map(val_id):\n",
    "            mask_float = (curr_mask == val_id).astype(np.float32)\n",
    "            # Interpolation is faster on GPU\n",
    "            t = torch.from_numpy(mask_float).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "            prob = F.interpolate(t, size=(hf, wf), mode='bilinear', align_corners=False).flatten()\n",
    "            return prob.cpu() # Transfer back to CPU after computation\n",
    "\n",
    "        prob_fg = get_prob_map(1)\n",
    "        prob_joint = get_prob_map(2)\n",
    "        prob_struct = get_prob_map(3)\n",
    "        \n",
    "        # 4. Selection Logic (Boolean Indexing)\n",
    "        # FG: High seepage probability, avoiding joint interference\n",
    "        idx_fg = (prob_fg > 0.15) & (prob_joint < 0.10)\n",
    "        \n",
    "        # Joint: Confirmed joints (Hard negative samples)\n",
    "        idx_joint = (prob_joint > 0.10)\n",
    "        \n",
    "        # Struct: Confirmed structural elements\n",
    "        idx_struct = (prob_struct > 0.50)\n",
    "        \n",
    "        # Normal: Clean background\n",
    "        idx_normal = (prob_fg < 0.05) & (prob_joint < 0.05) & (prob_struct < 0.05)\n",
    "        \n",
    "        # 5. Data Collection\n",
    "        if idx_fg.any():     feats_fg.append(feat_map[idx_fg])\n",
    "        if idx_joint.any():  feats_joint.append(feat_map[idx_joint])\n",
    "        if idx_struct.any(): feats_struct.append(feat_map[idx_struct])\n",
    "        \n",
    "        # Normal Downsampling (Prevent memory overflow: max 2000 patches per image)\n",
    "        curr_norm = feat_map[idx_normal]\n",
    "        if curr_norm.shape[0] > 2000:\n",
    "            perm = torch.randperm(curr_norm.shape[0])[:2000]\n",
    "            curr_norm = curr_norm[perm]\n",
    "        if curr_norm.shape[0] > 0:\n",
    "            feats_normal.append(curr_norm)\n",
    "\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e40ea-384a-4636-92cf-e18baacf1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 6: MLP Training (Enforced Gradients + Balanced Augmentation) ====\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MLP_SAVE_PATH = \"./classifier_mlp_focal.pth\"\n",
    "SEED          = 3407\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE    = 4096\n",
    "EPOCHS        = 150   \n",
    "LR            = 0.0005\n",
    "HIDDEN_DIM    = 256\n",
    "\n",
    "# ‚≠ê Critical Fix 1: Force enable global gradients (prevents issues if a previous cell disabled them)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "print(f\"üì• Preparing Balanced Data...\")\n",
    "if 'feats_fg' not in locals(): raise RuntimeError(\"Please run Cell 6 first.\")\n",
    "\n",
    "def cat_feats(feat_list):\n",
    "    if not feat_list: return torch.zeros((0, 1024))\n",
    "    # Ensure conversion to float32 to prevent precision issues if DINO output is float16\n",
    "    return torch.cat(feat_list, dim=0).float().cpu()\n",
    "\n",
    "# Raw data concatenation\n",
    "t_fg     = cat_feats(feats_fg)\n",
    "t_joint  = cat_feats(feats_joint)\n",
    "t_struct = cat_feats(feats_struct)\n",
    "t_normal = cat_feats(feats_normal)\n",
    "\n",
    "print(f\"   [Raw] FG: {len(t_fg)}, Joint: {len(t_joint)}, Struct: {len(t_struct)}, Normal: {len(t_normal)}\")\n",
    "\n",
    "# üî• Strategy 1: Joint Oversampling\n",
    "if len(t_joint) > 0:\n",
    "    repeat_factor = len(t_fg) // len(t_joint)\n",
    "    if repeat_factor > 1:\n",
    "        print(f\"   ‚ö° Oversampling Joints by {repeat_factor}x ...\")\n",
    "        t_joint = t_joint.repeat(repeat_factor, 1)\n",
    "\n",
    "# üî• Strategy 2: Background Downsampling\n",
    "if len(t_normal) > 8000:\n",
    "    perm = torch.randperm(len(t_normal))[:8000]\n",
    "    t_normal = t_normal[perm]\n",
    "    print(f\"   ‚úÇÔ∏è Downsampling Normal to {len(t_normal)}...\")\n",
    "\n",
    "# Merge datasets\n",
    "X = torch.cat([t_fg, t_joint, t_struct, t_normal], dim=0)\n",
    "# Labels: FG=1, Others=0\n",
    "y = torch.cat([\n",
    "    torch.ones(len(t_fg)), \n",
    "    torch.zeros(len(t_joint)),\n",
    "    torch.zeros(len(t_struct)),\n",
    "    torch.zeros(len(t_normal))\n",
    "], dim=0).long()\n",
    "\n",
    "print(f\"   [Final] Training Set: {len(X)} samples.\")\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# --- 3. Model Definition ---\n",
    "class PatchClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super(PatchClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "print(f\"üöÄ Training Enhanced MLP...\")\n",
    "\n",
    "model = PatchClassifier(input_dim=1024, hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
    "model.train() # Ensure training mode\n",
    "\n",
    "# Gradient check (debugging)\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"‚ö†Ô∏è Warning: {name} is frozen!\")\n",
    "        param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) \n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    correct = 0; total = 0\n",
    "    \n",
    "    for bx, by in loader:\n",
    "        bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "        \n",
    "        # üî• Strategy 3: Feature Noise Injection (Augmentation)\n",
    "        if model.training:\n",
    "            noise = torch.randn_like(bx) * 0.02\n",
    "            bx = bx + noise\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        logits = model(bx)\n",
    "        \n",
    "        # Loss Calculation\n",
    "        loss = criterion(logits, by)\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == by).sum().item()\n",
    "        total += by.size(0)\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        acc = correct / total\n",
    "        print(f\"   Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(loader):.5f} | Acc: {acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), MLP_SAVE_PATH)\n",
    "print(f\"‚úÖ Enhanced Model saved to: {MLP_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0342392-2937-4364-b51d-7991afceee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 7: Final Inference (Fixed Threshold 0.6 + TTA) ====\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import skimage.morphology as morph\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MLP_PATH       = \"./classifier_mlp_focal.pth\" # Must match training path in Cell 3\n",
    "IMAGE_SIZE     = 768\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ‚≠ê Locked optimal threshold\n",
    "FINAL_THRESHOLD = 0.60\n",
    "\n",
    "# --- 2. Model Definition (Matching 3-layer MLP architecture) ---\n",
    "print(f\"üß† Loading MLP from {MLP_PATH}...\")\n",
    "\n",
    "class PatchClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super(PatchClassifier, self).__init__()\n",
    "        # 3-layer MLP: 1024 -> 512 -> 256 -> 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate and load weights\n",
    "model = PatchClassifier(input_dim=1024, hidden_dim=256).to(DEVICE)\n",
    "\n",
    "if not os.path.exists(MLP_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå Model file not found: {MLP_PATH}\")\n",
    "\n",
    "try:\n",
    "    state_dict = torch.load(MLP_PATH, map_location=DEVICE, weights_only=True)\n",
    "except:\n",
    "    state_dict = torch.load(MLP_PATH, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"‚úÖ MLP Model Loaded Successfully.\")\n",
    "\n",
    "# --- 3. Dataset Preparation ---\n",
    "if 'ValDataset' not in locals():\n",
    "    class ValDataset:\n",
    "        def __init__(self, img_dir, ann_file):\n",
    "            self.img_dir = img_dir; self.coco = COCO(ann_file)\n",
    "            self.img_ids = self.coco.getImgIds()\n",
    "            self.data = [self.coco.loadImgs(i)[0] for i in self.img_ids if os.path.exists(os.path.join(img_dir, self.coco.loadImgs(i)[0]['file_name']))]\n",
    "        def get_data(self, idx):\n",
    "            info = self.data[idx]\n",
    "            img = Image.open(os.path.join(self.img_dir, info['file_name'])).convert(\"RGB\")\n",
    "            mask = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n",
    "            for ann in self.coco.loadAnns(self.coco.getAnnIds(imgIds=info['id'])):\n",
    "                if ann.get('category_id') == 1: mask = np.maximum(mask, self.coco.annToMask(ann))\n",
    "            return img, mask, info['file_name']\n",
    "\n",
    "val_dataset = ValDataset(VAL_IMG_DIR, VAL_ANN_FILE)\n",
    "\n",
    "# --- 4. TTA Inference Functions ---\n",
    "def predict_single(img_pil, extractor, mlp_model):\n",
    "    w_orig, h_orig = img_pil.size\n",
    "    img_t = extractor.preprocess(img_pil, IMAGE_SIZE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # DINO Feature Extraction\n",
    "        feat = extractor.model.get_intermediate_layers(img_t, n=1, reshape=True, norm=True)[0]\n",
    "        \n",
    "        # ‚úÖ Normalize features for consistency\n",
    "        feat = F.normalize(feat, dim=1)\n",
    "\n",
    "        B, D, H, W = feat.shape\n",
    "        # [1, 1024, H, W] -> [H*W, 1024]\n",
    "        feat_flat = feat.permute(0, 2, 3, 1).reshape(-1, D)\n",
    "        \n",
    "        logits = mlp_model(feat_flat)\n",
    "        prob = F.softmax(logits, dim=1)[:, 1].reshape(H, W)\n",
    "        \n",
    "        # Upsampling to original image resolution\n",
    "        prob_high = F.interpolate(\n",
    "            prob.unsqueeze(0).unsqueeze(0),\n",
    "            size=(h_orig, w_orig), mode='bilinear', align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "    return prob_high\n",
    "\n",
    "def predict_with_tta(img_pil, extractor, mlp_model):\n",
    "    # 1. Original Image Pass\n",
    "    p1 = predict_single(img_pil, extractor, mlp_model)\n",
    "    # 2. Horizontal Flip Pass (Test Time Augmentation)\n",
    "    p2 = np.fliplr(predict_single(img_pil.transpose(Image.FLIP_LEFT_RIGHT), extractor, mlp_model))\n",
    "    # 3. Ensemble Averaging\n",
    "    return (p1 + p2) / 2.0\n",
    "\n",
    "# --- 5. Execution (Applying 0.6 Threshold) ---\n",
    "print(f\"üöÄ Running Inference (TTA + Threshold={FINAL_THRESHOLD})...\")\n",
    "results = []\n",
    "ious = []\n",
    "mious = []\n",
    "\n",
    "pbar = tqdm(range(len(val_dataset.data)))\n",
    "for idx in pbar:\n",
    "    img, mask_np, fname = val_dataset.get_data(idx)\n",
    "    gt = (mask_np > 0).astype(np.uint8)\n",
    "    \n",
    "    # Generate probability map\n",
    "    score = predict_with_tta(img, extractor, model)\n",
    "    \n",
    "    # 1. Thresholding\n",
    "    pred = (score > FINAL_THRESHOLD).astype(np.uint8)\n",
    "    \n",
    "    # 2. Morphological Post-processing (Denoising)\n",
    "    if pred.sum() > 0:\n",
    "        pred = morph.binary_opening(pred, morph.disk(1)).astype(np.uint8)\n",
    "        pred = morph.remove_small_objects(pred.astype(bool), min_size=30).astype(np.uint8)\n",
    "        \n",
    "    # 3. Metrics Computation\n",
    "    inter = ((pred==1) & (gt==1)).sum()\n",
    "    union = ((pred==1) | (gt==1)).sum()\n",
    "    iou = inter / (union + 1e-6)\n",
    "    \n",
    "    # mIoU (FG IoU + BG IoU) / 2\n",
    "    inter_bg = ((pred==0) & (gt==0)).sum()\n",
    "    union_bg = ((pred==0) | (gt==0)).sum()\n",
    "    curr_miou = (iou + inter_bg/(union_bg+1e-6)) / 2.0\n",
    "    \n",
    "    ious.append(iou)\n",
    "    mious.append(curr_miou)\n",
    "    \n",
    "    results.append({\n",
    "        'name': fname, 'img': img, 'gt': gt, \n",
    "        'pred': pred, 'score': score, 'iou': iou\n",
    "    })\n",
    "    \n",
    "    pbar.set_postfix({'FG_IoU': f\"{np.mean(ious):.4f}\"})\n",
    "\n",
    "# --- 6. Final Statistics & Visualization ---\n",
    "print(f\"\\nüèÜ Final Results (Threshold={FINAL_THRESHOLD}):\")\n",
    "print(f\"   FG IoU : {np.mean(ious):.4f}\")\n",
    "print(f\"   mIoU   : {np.mean(mious):.4f}\")\n",
    "\n",
    "# Sort results by IoU for case analysis\n",
    "results.sort(key=lambda x: x['iou'])\n",
    "\n",
    "def plot_cases_final(cases, title):\n",
    "    if not cases: return\n",
    "    n = len(cases); plt.figure(figsize=(20, 4*n)); plt.suptitle(title, fontsize=20, y=1.005)\n",
    "    for i, res in enumerate(cases):\n",
    "        img = res['img']; gt = res['gt']; pred = res['pred']; score = res['score']\n",
    "        h, w = gt.shape; vis = np.zeros((h, w, 3))\n",
    "        \n",
    "        # Color coding: Green=TP, Yellow=FN(Miss), Red=FP(False Alarm)\n",
    "        vis[(gt==1) & (pred==1)] = [0, 1, 0] \n",
    "        vis[(gt==1) & (pred==0)] = [1, 1, 0] \n",
    "        vis[(gt==0) & (pred==1)] = [1, 0, 0] \n",
    "        \n",
    "        img_np = np.array(img) / 255.0\n",
    "        mask_any = (gt==1) | (pred==1)\n",
    "        overlay = img_np.copy()\n",
    "        if mask_any.sum() > 0: overlay[mask_any] = img_np[mask_any]*0.4 + vis[mask_any]*0.6\n",
    "        \n",
    "        plt.subplot(n, 4, i*4+1); plt.imshow(img); plt.title(f\"{res['name']}\\nIoU: {res['iou']:.3f}\"); plt.axis('off')\n",
    "        plt.subplot(n, 4, i*4+2); plt.imshow(gt, cmap='gray'); plt.title(\"GT\"); plt.axis('off')\n",
    "        plt.subplot(n, 4, i*4+3); plt.imshow(score, cmap='jet', vmin=0, vmax=1); plt.title(f\"Score Max:{score.max():.2f}\"); plt.axis('off')\n",
    "        plt.subplot(n, 4, i*4+4); plt.imshow(overlay); plt.title(\"G=TP, Y=Miss, R=FP\"); plt.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "print(\"üîç Analyzing Worst Cases\")\n",
    "plot_cases_final(results[:5], f\"Worst Cases (Threshold={FINAL_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8536f-dd96-40d8-915e-63ea8c63db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509c83a-3d04-4e0b-a65c-128a5482a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 8: Final Inference (Fixed Threshold 0.6 + TTA) ====\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "PROTO_PATH    = \"./proto_fullres.npz\"\n",
    "MLP_PATH       = \"./classifier_mlp_focal.pth\"\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. Independent TTA (Test-Time Augmentation) Configurations ---\n",
    "\n",
    "# [Branch 1: Prototype-based TTA Strategy]\n",
    "# Based on the Fixed-Base logic from previous iterations\n",
    "# Format: (scale, flip, rotation)\n",
    "PROTO_TTA_CONFIG = [\n",
    "    (1.0, False, 0),   # 1. Baseline\n",
    "    (1.0, True, 0),    # 2. Horizontal Flip (Robustness)\n",
    "    # (0.8, False, 0), # Optional: Downscale for global context\n",
    "]\n",
    "PROTO_BASE_SIZE = (1200, 1600) # (H, W) Base resolution\n",
    "\n",
    "# [Branch 2: MLP-based TTA Strategy]\n",
    "# Standard flip augmentation at fixed 768 resolution\n",
    "MLP_TTA_OPS = ['orig', 'flip'] \n",
    "\n",
    "# --- 3. Model Definition ---\n",
    "class PatchClassifier(nn.Module):\n",
    "    \"\"\"Simple MLP Classifier for feature patches.\"\"\"\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super(PatchClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class HybridEnsemblePredictor:\n",
    "    \"\"\"Ensemble model combining Prototype similarity and MLP classification.\"\"\"\n",
    "    def __init__(self, extractor, proto_path, mlp_path, device):\n",
    "        self.device = device\n",
    "        self.extractor = extractor\n",
    "        self.patch_size = 16\n",
    "        \n",
    "        # Load Prototypes (High-Res Branch)\n",
    "        print(f\"üîπ Loading Branch 1 (Proto High-Res)...\")\n",
    "        pd = np.load(proto_path)\n",
    "        self.fg_p = torch.from_numpy(pd['fg']).float().to(device)\n",
    "        self.bg_p = torch.from_numpy(pd['bg']).float().to(device)\n",
    "        self.fg_p = F.normalize(self.fg_p, dim=1)\n",
    "        self.bg_p = F.normalize(self.bg_p, dim=1)\n",
    "        self.topk_fg = 64\n",
    "        self.topk_bg = 96\n",
    "        \n",
    "        # Load MLP Classifier (Low-Res Branch)\n",
    "        print(f\"üî∏ Loading Branch 2 (MLP Low-Res)...\")\n",
    "        self.mlp = PatchClassifier().to(device)\n",
    "        try:\n",
    "            state_dict = torch.load(mlp_path, map_location=device, weights_only=True)\n",
    "        except:\n",
    "            state_dict = torch.load(mlp_path, map_location=device)\n",
    "        self.mlp.load_state_dict(state_dict)\n",
    "        self.mlp.eval()\n",
    "        self.extractor.model.eval()\n",
    "\n",
    "    # ==================================================\n",
    "    # Branch 1: Prototype Independent TTA (Fixed Base)\n",
    "    # ==================================================\n",
    "    def _infer_proto_single(self, img_pil, scale, flip, rotation):\n",
    "        w_orig, h_orig = img_pil.size\n",
    "        BASE_H, BASE_W = PROTO_BASE_SIZE\n",
    "        \n",
    "        # 1. Calculate target dimensions aligned with patch size\n",
    "        h_target = (int(BASE_H * scale) // 16) * 16\n",
    "        w_target = (int(BASE_W * scale) // 16) * 16\n",
    "        \n",
    "        # Adaptive Orientation (Portrait vs Landscape)\n",
    "        if h_orig > w_orig:\n",
    "            h_in, w_in = max(h_target, w_target), min(h_target, w_target)\n",
    "        else:\n",
    "            h_in, w_in = min(h_target, w_target), max(h_target, w_target)\n",
    "            \n",
    "        # 2. Apply TTA Transformations\n",
    "        img_input = img_pil\n",
    "        if flip: img_input = ImageOps.mirror(img_input)\n",
    "        if rotation != 0: img_input = img_input.rotate(rotation, resample=Image.BICUBIC)\n",
    "        \n",
    "        # 3. Resize & Inference\n",
    "        img_input = img_input.resize((w_in, h_in), Image.BICUBIC)\n",
    "        img_t = self.extractor.preprocess(img_input, size=None) # Native mode processing\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.extractor.model.get_intermediate_layers(img_t, n=1, reshape=True, norm=True)[0]\n",
    "            B, D, Hf, Wf = out.shape\n",
    "            \n",
    "            feat = F.normalize(out.squeeze(0).permute(1, 2, 0).reshape(-1, D), dim=1)\n",
    "            \n",
    "            # Compute similarity with prototypes\n",
    "            sim_fg = torch.mm(feat, self.fg_p.t())\n",
    "            sim_bg = torch.mm(feat, self.bg_p.t())\n",
    "            \n",
    "            val_fg, _ = sim_fg.topk(min(self.topk_fg, sim_fg.shape[1]), dim=1)\n",
    "            val_bg, _ = sim_bg.topk(min(self.topk_bg, sim_bg.shape[1]), dim=1)\n",
    "            \n",
    "            logits = torch.cat([val_fg, val_bg], dim=1)\n",
    "            probs = F.softmax(logits / 0.07, dim=1)\n",
    "            \n",
    "            score_fg = probs[:, :val_fg.shape[1]].sum(dim=1).view(1, 1, Hf, Wf)\n",
    "            \n",
    "            # 4. Upsample back to original image resolution\n",
    "            prob_map = F.interpolate(score_fg, size=(h_orig, w_orig), mode='bilinear', align_corners=False).squeeze().cpu().numpy()\n",
    "            \n",
    "            # 5. Inverse TTA Transformations\n",
    "            if rotation != 0:\n",
    "                 prob_map = np.array(Image.fromarray((prob_map * 255).astype(np.uint8)).rotate(-rotation, resample=Image.NEAREST, expand=False))/255.0\n",
    "            if flip:\n",
    "                prob_map = np.fliplr(prob_map)\n",
    "                \n",
    "            return prob_map\n",
    "\n",
    "    def _get_proto_tta_result(self, img_pil):\n",
    "        \"\"\"Aggregates TTA passes for the Prototype branch.\"\"\"\n",
    "        accum_map = np.zeros((img_pil.size[1], img_pil.size[0]), dtype=np.float32)\n",
    "        count = 0\n",
    "        for scale, flip, rot in PROTO_TTA_CONFIG:\n",
    "            res = self._infer_proto_single(img_pil, scale, flip, rot)\n",
    "            accum_map += res\n",
    "            count += 1\n",
    "        return accum_map / count\n",
    "\n",
    "    # ==================================================\n",
    "    # Branch 2: MLP Independent TTA (Fixed 768 Resolution)\n",
    "    # ==================================================\n",
    "    def _infer_mlp_single(self, img_pil, flip):\n",
    "        w_orig, h_orig = img_pil.size\n",
    "        # Apply TTA Flip\n",
    "        if flip: img_pil = ImageOps.mirror(img_pil)\n",
    "        \n",
    "        # Forced 768 resolution for consistency\n",
    "        img_t = self.extractor.preprocess(img_pil, size=768)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feat = self.extractor.model.get_intermediate_layers(img_t, n=1, reshape=True, norm=True)[0]\n",
    "            feat = F.normalize(feat, dim=1)\n",
    "            B, D, Hf, Wf = feat.shape\n",
    "            \n",
    "            logits = self.mlp(feat.permute(0, 2, 3, 1).reshape(-1, D))\n",
    "            score_fg = F.softmax(logits, dim=1)[:, 1].view(1, 1, Hf, Wf)\n",
    "            \n",
    "            # Upsample\n",
    "            prob_map = F.interpolate(score_fg, size=(h_orig, w_orig), mode='bilinear', align_corners=False).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Inverse TTA Flip\n",
    "            if flip: prob_map = np.fliplr(prob_map)\n",
    "            return prob_map\n",
    "\n",
    "    def _get_mlp_tta_result(self, img_pil):\n",
    "        \"\"\"Aggregates TTA passes for the MLP branch.\"\"\"\n",
    "        p1 = self._infer_mlp_single(img_pil, flip=False)\n",
    "        p2 = self._infer_mlp_single(img_pil, flip=True)\n",
    "        return (p1 + p2) / 2.0\n",
    "\n",
    "    # ==================================================\n",
    "    # Fusion Interface\n",
    "    # ==================================================\n",
    "    def predict_fusion_components(self, img_pil):\n",
    "        \"\"\"\n",
    "        Executes TTA for both branches and returns two independent probability maps.\n",
    "        Weights are not applied here to allow for external weight search loops.\n",
    "        \"\"\"\n",
    "        p_proto = self._get_proto_tta_result(img_pil)\n",
    "        p_mlp   = self._get_mlp_tta_result(img_pil)\n",
    "        return p_proto, p_mlp\n",
    "\n",
    "# Initialization\n",
    "if 'extractor' in locals():\n",
    "    ensemble_model = HybridEnsemblePredictor(extractor, PROTO_PATH, MLP_PATH, DEVICE)\n",
    "    print(\"‚úÖ Model initialized with Independent Dual-TTA.\")\n",
    "else:\n",
    "    print(\"‚ùå Please load the 'extractor' first (Infrastructure Cell).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161df73c-1d66-49b0-8ed6-2fdc4231a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell9: Alpha & Threshold Joint Grid Search ====\n",
    "\n",
    "import numpy as np\n",
    "import skimage.morphology as morph\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Define Search Space ---\n",
    "SEARCH_ALPHAS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# üî• Key Improvement: Thresholds are no longer fixed; testing three critical points dynamically\n",
    "SEARCH_THRESHOLDS = [0.50, 0.55, 0.60] \n",
    "\n",
    "# Morphological Post-processing (Settings proven effective for MLP)\n",
    "USE_OPENING = True \n",
    "MIN_SIZE    = 30\n",
    "\n",
    "print(f\"üöÄ Starting Joint Search (Alpha x Threshold)...\")\n",
    "print(f\"   Goal: Identify optimal scores across the entire ensemble spectrum.\")\n",
    "\n",
    "# Ensure cache_results exists (Run the previous caching cell if missing)\n",
    "if 'cache_results' not in locals() or len(cache_results) == 0:\n",
    "    raise RuntimeError(\"‚ùå 'cache_results' not found! Please run the caching cell first.\")\n",
    "\n",
    "best_global_miou = 0\n",
    "best_global_alpha = 0\n",
    "best_global_th = 0\n",
    "\n",
    "print(f\"{'Alpha':<6} | {'Best Th':<8} | {'mIoU':<8} | {'FG IoU':<8} | {'Notes'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for alpha in SEARCH_ALPHAS:\n",
    "    # Track the best performance for the current Alpha\n",
    "    current_alpha_best_miou = 0\n",
    "    current_alpha_best_th   = 0\n",
    "    current_alpha_fg        = 0\n",
    "    \n",
    "    # --- Inner Loop: Finding the best threshold for the current mixture ratio ---\n",
    "    for th in SEARCH_THRESHOLDS:\n",
    "        mious = []\n",
    "        fg_ious = []\n",
    "        \n",
    "        for item in cache_results:\n",
    "            # 1. Linear Fusion (Weighted Averaging)\n",
    "            score = alpha * item['p_proto'] + (1 - alpha) * item['p_mlp']\n",
    "            \n",
    "            # 2. Dynamic Thresholding\n",
    "            pred = (score > th).astype(np.uint8)\n",
    "            \n",
    "            # 3. Post-processing (Opening + Small Object Removal)\n",
    "            if pred.sum() > 0:\n",
    "                if USE_OPENING:\n",
    "                    pred = morph.binary_opening(pred, morph.disk(1)).astype(np.uint8)\n",
    "                pred = morph.remove_small_objects(pred.astype(bool), min_size=MIN_SIZE).astype(np.uint8)\n",
    "            \n",
    "            # 4. Metric Calculation\n",
    "            gt = item['gt']\n",
    "            i = ((pred==1) & (gt==1)).sum()\n",
    "            u = ((pred==1) | (gt==1)).sum()\n",
    "            iou = i / (u + 1e-6)\n",
    "            \n",
    "            bg_i = ((pred==0) & (gt==0)).sum()\n",
    "            bg_u = ((pred==0) | (gt==0)).sum()\n",
    "            miou = (iou + bg_i/(bg_u+1e-6)) / 2.0\n",
    "            \n",
    "            mious.append(miou)\n",
    "            fg_ious.append(iou)\n",
    "        \n",
    "        # Check if this threshold is the best for the current Alpha\n",
    "        avg_miou = np.mean(mious)\n",
    "        if avg_miou > current_alpha_best_miou:\n",
    "            current_alpha_best_miou = avg_miou\n",
    "            current_alpha_best_th   = th\n",
    "            current_alpha_fg        = np.mean(fg_ious)\n",
    "\n",
    "    # Print the best result for the current Alpha\n",
    "    # Marker: Star indicates a new global high for mIoU\n",
    "    marker = \"‚≠ê\" if current_alpha_best_miou > best_global_miou else \"\"\n",
    "    \n",
    "    print(f\"{alpha:<6} | {current_alpha_best_th:<8} | {current_alpha_best_miou:.4f}   | {current_alpha_fg:.4f}   | {marker}\")\n",
    "    \n",
    "    # Update Global Optimum\n",
    "    if current_alpha_best_miou > best_global_miou:\n",
    "        best_global_miou  = current_alpha_best_miou\n",
    "        best_global_alpha = alpha\n",
    "        best_global_th    = current_alpha_best_th\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üèÜ GLOBAL BEST CONFIGURATION:\")\n",
    "print(f\"   Alpha     : {best_global_alpha}\")\n",
    "print(f\"   Threshold : {best_global_th}\")\n",
    "print(f\"   Max mIoU  : {best_global_miou:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93a192-9b3f-480b-881e-b571c95dcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 10: GRAND FINAL PREDICTION (TTA + Alpha 0.6 + Th 0.55 + CRF) ====\n",
    "# Dependency: !pip install pydensecrf\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_softmax\n",
    "from PIL import Image\n",
    "import skimage.morphology as morph\n",
    "\n",
    "# ==========================================\n",
    "# 1. Champion Configuration (Derived from Joint Search)\n",
    "# ==========================================\n",
    "BEST_ALPHA     = 0.60   # üèÜ Optimal Weight (60% Proto / 40% MLP)\n",
    "FINAL_TH       = 0.55   # üèÜ Optimal Threshold (Balanced point)\n",
    "USE_CRF        = True   # Enable DenseCRF for boundary refinement\n",
    "\n",
    "# Morphological Post-processing (Consistent with search phase)\n",
    "USE_OPENING    = True\n",
    "MIN_SIZE       = 30\n",
    "\n",
    "# DenseCRF Parameters (High-precision configuration)\n",
    "CRF_ITER       = 5\n",
    "CRF_POS_W      = 3; CRF_POS_XY  = 3\n",
    "CRF_Bi_W       = 5; CRF_Bi_XY   = 50; CRF_Bi_RGB  = 10\n",
    "\n",
    "print(f\"üöÄ Starting Grand Final Inference...\")\n",
    "print(f\"   Config: Alpha={BEST_ALPHA} | Th={FINAL_TH} | CRF=ON\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Core CRF Function\n",
    "# ==========================================\n",
    "def apply_dense_crf(img_np, prob_map):\n",
    "    h, w = prob_map.shape\n",
    "    # Stack probabilities for background and foreground\n",
    "    prob_stack = np.stack([1 - prob_map, prob_map], axis=0)\n",
    "    prob_stack = np.clip(prob_stack, 1e-6, 1.0)\n",
    "    \n",
    "    d = dcrf.DenseCRF2D(w, h, 2)\n",
    "    U = unary_from_softmax(prob_stack)\n",
    "    d.setUnaryEnergy(U)\n",
    "\n",
    "    # 1. Pairwise Gaussian Potential (Smoothness)\n",
    "    d.addPairwiseGaussian(sxy=(CRF_POS_XY, CRF_POS_XY), compat=CRF_POS_W, \n",
    "                          kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # 2. Pairwise Bilateral Potential (Appearance-based refinement)\n",
    "    d.addPairwiseBilateral(sxy=(CRF_Bi_XY, CRF_Bi_XY), srgb=(CRF_Bi_RGB, CRF_Bi_RGB, CRF_Bi_RGB), \n",
    "                           rgbim=img_np, compat=CRF_Bi_W, \n",
    "                           kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    Q = d.inference(CRF_ITER)\n",
    "    return np.argmax(Q, axis=0).reshape((h, w)).astype(np.uint8)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Execution of Final Inference\n",
    "# ==========================================\n",
    "if 'ds_search' not in locals():\n",
    "    ds_final = SearchDataset(VAL_IMG_DIR, VAL_ANN_FILE)\n",
    "else:\n",
    "    ds_final = ds_search\n",
    "\n",
    "results_final = []\n",
    "ious_base = []\n",
    "ious_crf = []\n",
    "mious_crf = []\n",
    "\n",
    "pbar = tqdm(range(len(ds_final.img_ids)))\n",
    "\n",
    "for idx in pbar:\n",
    "    img_pil, gt = ds_final.get_data(idx)\n",
    "    img_np = np.array(img_pil)\n",
    "    \n",
    "    # 1. Get Dual-Branch TTA Probabilities\n",
    "    p_proto, p_mlp = ensemble_model.predict_components(img_pil)\n",
    "    \n",
    "    # 2. Fusion (Alpha=0.6)\n",
    "    score_map = BEST_ALPHA * p_proto + (1 - BEST_ALPHA) * p_mlp\n",
    "    \n",
    "    # --- A. Base Prediction (Ensemble Only) ---\n",
    "    pred_base = (score_map > FINAL_TH).astype(np.uint8)\n",
    "    \n",
    "    # Apply morphological cleanup\n",
    "    if pred_base.sum() > 0:\n",
    "        if USE_OPENING:\n",
    "            pred_base = morph.binary_opening(pred_base, morph.disk(1)).astype(np.uint8)\n",
    "        pred_base = morph.remove_small_objects(pred_base.astype(bool), min_size=MIN_SIZE).astype(np.uint8)\n",
    "    \n",
    "    # Base Metric\n",
    "    i_b = ((pred_base==1) & (gt==1)).sum()\n",
    "    u_b = ((pred_base==1) | (gt==1)).sum()\n",
    "    iou_base_val = i_b / (u_b + 1e-6)\n",
    "    ious_base.append(iou_base_val)\n",
    "\n",
    "    # --- B. CRF Prediction (Refinement) ---\n",
    "    if USE_CRF:\n",
    "        # CRF operates directly on the probability map (threshold-free)\n",
    "        pred_crf = apply_dense_crf(img_np, score_map)\n",
    "        # Safety cleanup for CRF artifacts\n",
    "        if pred_crf.sum() > 0:\n",
    "            pred_crf = morph.remove_small_objects(pred_crf.astype(bool), min_size=30).astype(np.uint8)\n",
    "    else:\n",
    "        pred_crf = pred_base\n",
    "        \n",
    "    # CRF Metric\n",
    "    i_c = ((pred_crf==1) & (gt==1)).sum()\n",
    "    u_c = ((pred_crf==1) | (gt==1)).sum()\n",
    "    curr_iou_crf = i_c / (u_c + 1e-6)\n",
    "    \n",
    "    bg_i = ((pred_crf==0) & (gt==0)).sum()\n",
    "    bg_u = ((pred_crf==0) | (gt==0)).sum()\n",
    "    curr_miou_crf = (curr_iou_crf + bg_i/(bg_u+1e-6)) / 2.0\n",
    "    \n",
    "    ious_crf.append(curr_iou_crf)\n",
    "    mious_crf.append(curr_miou_crf)\n",
    "    \n",
    "    # Collect data for visualization (Sample bad cases or periodic samples)\n",
    "    if idx % 20 == 0 or curr_iou_crf < 0.6: \n",
    "        results_final.append({\n",
    "            'img': img_np, 'gt': gt, 'score': score_map,\n",
    "            'pred_base': pred_base, 'pred_crf': pred_crf,\n",
    "            'iou_base': iou_base_val, 'iou_crf': curr_iou_crf\n",
    "        })\n",
    "        \n",
    "    pbar.set_postfix({\n",
    "        'Base': f\"{np.mean(ious_base):.4f}\", \n",
    "        'CRF': f\"{np.mean(ious_crf):.4f}\"\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 4. Final Performance Report\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üèÜ FINAL PERFORMANCE REPORT\")\n",
    "print(f\"   Ensemble   : Alpha={BEST_ALPHA}, Th={FINAL_TH}\")\n",
    "print(f\"   Morphology : Opening=True, MinSize={MIN_SIZE}\")\n",
    "print(f\"   CRF        : Enabled (Iter={CRF_ITER})\")\n",
    "print(f\"   ----------------------------------------\")\n",
    "print(f\"   Base FG IoU : {np.mean(ious_base):.4f}\")\n",
    "print(f\"   Base mIoU   : {(np.mean(ious_base) + 0.9590)/2:.4f} (Est.)\") \n",
    "print(f\"   ----------------------------------------\")\n",
    "print(f\"   CRF  FG IoU : {np.mean(ious_crf):.4f}\")\n",
    "print(f\"   CRF  mIoU   : {np.mean(mious_crf):.4f}  <-- OFFICIAL FINAL SCORE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Visualization of Top Improvements\n",
    "# ==========================================\n",
    "\n",
    "# 1. Calculate improvements\n",
    "for res in results_final:\n",
    "    res['diff'] = res['iou_crf'] - res['iou_base']\n",
    "\n",
    "# 2. Sort by improvement gain\n",
    "top_gains = sorted(results_final, key=lambda x: x['diff'], reverse=True)[:3]\n",
    "\n",
    "def plot_final_comparison(cases, title):\n",
    "    if not cases: return\n",
    "    n = len(cases)\n",
    "    plt.figure(figsize=(18, 5*n))\n",
    "    plt.suptitle(title, fontsize=20, y=1.01)\n",
    "    \n",
    "    for i, res in enumerate(cases):\n",
    "        # 1. Original Image\n",
    "        plt.subplot(n, 4, i*4+1)\n",
    "        plt.imshow(res['img'])\n",
    "        plt.title(f\"Original Image\", fontsize=12); plt.axis('off')\n",
    "        \n",
    "        # 2. Ground Truth\n",
    "        plt.subplot(n, 4, i*4+2)\n",
    "        plt.imshow(res['gt'], cmap='gray')\n",
    "        plt.title(\"Ground Truth\", fontsize=12); plt.axis('off')\n",
    "        \n",
    "        # 3. Base Prediction\n",
    "        plt.subplot(n, 4, i*4+3)\n",
    "        plt.imshow(res['pred_base'], cmap='gray')\n",
    "        plt.title(f\"Ensemble Only\\nIoU: {res['iou_base']:.3f}\", fontsize=12); plt.axis('off')\n",
    "        \n",
    "        # 4. CRF Prediction\n",
    "        plt.subplot(n, 4, i*4+4)\n",
    "        plt.imshow(res['pred_crf'], cmap='gray')\n",
    "        plt.title(f\"Ensemble + CRF\\nIoU: {res['iou_crf']:.3f}\\n(Gain: +{res['diff']:.3f})\", fontsize=12); plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"üåü Final Score Analysis:\")\n",
    "print(f\"   mIoU Reached: {np.mean(mious_crf):.4f}\")\n",
    "print(f\"   FG IoU Reached: {np.mean(ious_crf):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"üîç Visualizing Top 3 CRF Improvements (Where CRF saved the day)...\")\n",
    "plot_final_comparison(top_gains, \"Top 3 CRF Improvements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (web)",
   "language": "python",
   "name": "web"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
